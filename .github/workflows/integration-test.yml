name: Integration Test & Analysis

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: false
        default: 'comprehensive'
        type: choice
        options:
        - comprehensive
        - performance
        - correctness
        - all

jobs:
  comprehensive-test:
    name: Comprehensive Integration Test
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install maturin[patchelf] pytest pytest-cov

    - name: Build package
      run: maturin develop

    - name: Run comprehensive tests
      run: |
        # Create test results directory
        mkdir -p test-results
        
        # Run all tests with coverage
        python -m pytest tests/ -v --cov=fast_exif_reader --cov-report=xml --cov-report=html --junitxml=test-results/junit.xml
        
        # Run comprehensive benchmark if available
        if [ -f "benchmarks/comprehensive_benchmark.py" ]; then
          python benchmarks/comprehensive_benchmark.py --output test-results/benchmark_results.json
        fi
        
        # Run correctness tests if available
        if [ -f "tests/test_correctness.py" ]; then
          python -m pytest tests/test_correctness.py -v --junitxml=test-results/correctness.xml
        fi

    - name: Generate test summary
      run: |
        echo "# Test Results Summary" > test-results/summary.md
        echo "" >> test-results/summary.md
        echo "## Build Status" >> test-results/summary.md
        echo "- ✅ Package built successfully" >> test-results/summary.md
        echo "- ✅ All dependencies installed" >> test-results/summary.md
        echo "" >> test-results/summary.md
        echo "## Test Execution" >> test-results/summary.md
        echo "- Tests completed at: $(date)" >> test-results/summary.md
        echo "- Python version: $(python --version)" >> test-results/summary.md
        echo "- Rust version: $(rustc --version)" >> test-results/summary.md
        echo "" >> test-results/summary.md
        echo "## Coverage Report" >> test-results/summary.md
        if [ -f "coverage.xml" ]; then
          echo "- Coverage report generated" >> test-results/summary.md
        fi
        echo "" >> test-results/summary.md
        echo "## Artifacts" >> test-results/summary.md
        echo "- JUnit XML: test-results/junit.xml" >> test-results/summary.md
        echo "- Coverage XML: coverage.xml" >> test-results/summary.md
        echo "- Coverage HTML: htmlcov/" >> test-results/summary.md
        if [ -f "test-results/benchmark_results.json" ]; then
          echo "- Benchmark results: test-results/benchmark_results.json" >> test-results/summary.md
        fi

    - name: Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ github.run_number }}
        path: |
          test-results/
          coverage.xml
          htmlcov/
        retention-days: 30

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  performance-test:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'all'

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install maturin[patchelf] pytest

    - name: Build package (release)
      run: maturin develop --release

    - name: Run performance benchmarks
      run: |
        mkdir -p benchmark-results
        
        # Run all available benchmarks
        for benchmark in benchmarks/*.py; do
          if [ -f "$benchmark" ]; then
            echo "Running $(basename $benchmark)..."
            python "$benchmark" --output "benchmark-results/$(basename $benchmark .py).json" || echo "Benchmark $(basename $benchmark) failed"
          fi
        done
        
        # Generate performance summary
        echo "# Performance Benchmark Results" > benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        echo "## Execution Details" >> benchmark-results/summary.md
        echo "- Timestamp: $(date)" >> benchmark-results/summary.md
        echo "- Python version: $(python --version)" >> benchmark-results/summary.md
        echo "- Rust version: $(rustc --version)" >> benchmark-results/summary.md
        echo "- Build type: Release" >> benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        echo "## Available Results" >> benchmark-results/summary.md
        ls -la benchmark-results/*.json >> benchmark-results/summary.md 2>/dev/null || echo "No benchmark results found"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_number }}
        path: benchmark-results/
        retention-days: 30

  correctness-test:
    name: Correctness Validation
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'correctness' || github.event.inputs.test_type == 'all'

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install maturin[patchelf] pytest

    - name: Build package
      run: maturin develop

    - name: Run correctness tests
      run: |
        mkdir -p correctness-results
        
        # Run all correctness-related tests
        python -m pytest tests/ -k "correctness or validation" -v --junitxml=correctness-results/correctness.xml || true
        
        # Run any specific correctness scripts
        for script in test_*correctness*.py test_*validation*.py; do
          if [ -f "$script" ]; then
            echo "Running $script..."
            python "$script" --output "correctness-results/$(basename $script .py).json" || echo "$script failed"
          fi
        done
        
        # Generate correctness summary
        echo "# Correctness Test Results" > correctness-results/summary.md
        echo "" >> correctness-results/summary.md
        echo "## Test Execution" >> correctness-results/summary.md
        echo "- Timestamp: $(date)" >> correctness-results/summary.md
        echo "- Python version: $(python --version)" >> correctness-results/summary.md
        echo "- Rust version: $(rustc --version)" >> correctness-results/summary.md
        echo "" >> correctness-results/summary.md
        echo "## Results" >> correctness-results/summary.md
        if [ -f "correctness-results/correctness.xml" ]; then
          echo "- JUnit XML: correctness-results/correctness.xml" >> correctness-results/summary.md
        fi
        ls -la correctness-results/*.json >> correctness-results/summary.md 2>/dev/null || echo "No additional correctness results found"

    - name: Upload correctness results
      uses: actions/upload-artifact@v4
      with:
        name: correctness-results-${{ github.run_number }}
        path: correctness-results/
        retention-days: 30

  generate-report:
    name: Generate Integration Report
    runs-on: ubuntu-latest
    needs: [comprehensive-test, performance-test, correctness-test]
    if: always()

    steps:
    - uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate comprehensive report
      run: |
        mkdir -p final-report
        
        echo "# GitHub Actions Integration Report" > final-report/integration-report.md
        echo "" >> final-report/integration-report.md
        echo "## Workflow Summary" >> final-report/integration-report.md
        echo "- Workflow: ${{ github.workflow }}" >> final-report/integration-report.md
        echo "- Run ID: ${{ github.run_id }}" >> final-report/integration-report.md
        echo "- Run Number: ${{ github.run_number }}" >> final-report/integration-report.md
        echo "- Commit: ${{ github.sha }}" >> final-report/integration-report.md
        echo "- Branch: ${{ github.ref_name }}" >> final-report/integration-report.md
        echo "- Triggered by: ${{ github.event_name }}" >> final-report/integration-report.md
        echo "" >> final-report/integration-report.md
        
        echo "## Job Results" >> final-report/integration-report.md
        echo "- Comprehensive Test: ${{ needs.comprehensive-test.result }}" >> final-report/integration-report.md
        echo "- Performance Test: ${{ needs.performance-test.result }}" >> final-report/integration-report.md
        echo "- Correctness Test: ${{ needs.correctness-test.result }}" >> final-report/integration-report.md
        echo "" >> final-report/integration-report.md
        
        echo "## Available Artifacts" >> final-report/integration-report.md
        find . -name "*.md" -o -name "*.json" -o -name "*.xml" | head -20 >> final-report/integration-report.md
        
        echo "" >> final-report/integration-report.md
        echo "## Next Steps" >> final-report/integration-report.md
        echo "1. Review test results in artifacts" >> final-report/integration-report.md
        echo "2. Check coverage reports" >> final-report/integration-report.md
        echo "3. Analyze performance benchmarks" >> final-report/integration-report.md
        echo "4. Validate correctness test results" >> final-report/integration-report.md

    - name: Upload final report
      uses: actions/upload-artifact@v4
      with:
        name: integration-report-${{ github.run_number }}
        path: final-report/
        retention-days: 30

    - name: Comment on PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          let comment = `## 🔍 Integration Test Results\n\n`;
          comment += `**Workflow:** ${context.workflow}\n`;
          comment += `**Run:** [#${context.runNumber}](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n\n`;
          
          comment += `### Job Status\n`;
          comment += `- Comprehensive Test: ${context.payload.check_suite ? '✅' : '❌'}\n`;
          comment += `- Performance Test: ${context.payload.check_suite ? '✅' : '❌'}\n`;
          comment += `- Correctness Test: ${context.payload.check_suite ? '✅' : '❌'}\n\n`;
          
          comment += `### Artifacts\n`;
          comment += `- [Test Results](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n`;
          comment += `- [Benchmark Results](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n`;
          comment += `- [Integration Report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n\n`;
          
          comment += `*This comment was automatically generated by the integration test workflow.*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
